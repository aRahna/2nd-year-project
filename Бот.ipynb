{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a542ec34",
   "metadata": {},
   "source": [
    "## Проект бот-рифмоплет\n",
    "\n",
    "### Инструменты:\n",
    "- Python 3\n",
    "- Jupyter Notebook\n",
    "- HTML\n",
    "- CSS\n",
    "- Bootstrap\n",
    "- JQuery\n",
    "- Markdown\n",
    "- Цепи Маркова\n",
    "+ https://rustih.ru/dzhordzh-bajron/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0690441",
   "metadata": {},
   "source": [
    "### Начало работы: \n",
    "\n",
    "- поиск текстов для корпуса\n",
    "- скачивание и обработка для дальнейшего обучения\n",
    "\n",
    "кусочки кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "\n",
    "for page in self.soup.findAll(\"div\", class_ = \"nav-links\"):\n",
    "            page_links.append(str(page))\n",
    "        \n",
    "for page in page_links:\n",
    "    start_point = page.find(\"Последняя\", 0) - 20\n",
    "            new_start_point = page.find(\"page\", start_point) + len(\"page/\")\n",
    "            end_point = page.find(\"/\", new_start_point)\n",
    "            try:\n",
    "                page_value = int(page[new_start_point:end_point])\n",
    "                page_values.append(page_value)\n",
    "            except:\n",
    "                pass\n",
    "min_page = 0\n",
    "        for value in set(page_values):\n",
    "            if value > min_page:\n",
    "                min_page = value\n",
    "\n",
    "        page_links = []\n",
    "        for page_value in range(1, value + 1):\n",
    "            page_links.append(self.original_link + \"page/\" + str(page_value) + \"/\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        import bs4 as bs\n",
    "        import urllib.request\n",
    "        import time\n",
    "        import random\n",
    "        import io\n",
    "        \n",
    "        self._getPoems()\n",
    "        all_text = []\n",
    "        \n",
    "        for idx, link in enumerate(self.poem_links):\n",
    "            rng = random.randint(5, 15)\n",
    "            time.sleep(rng)\n",
    "            \n",
    "            page_source = urllib.request.urlopen(link).read()\n",
    "            soup = bs.BeautifulSoup(page_source, \"lxml\")\n",
    "            \n",
    "            for text in soup.findAll(\"div\", class_ = \"entry-content poem-text\"):\n",
    "                all_text.append(text.text)\n",
    "                \n",
    "        self.all_texts = all_text\n",
    "        \n",
    "        \n",
    "        file_name = \"poems \" + self.author + \".txt\"\n",
    "        with io.open(file_name, \"w\", encoding=\"utf-8\") as output:\n",
    "            for poem in self.all_texts:\n",
    "                output.write(poem + \"\\n\")\n",
    "                \n",
    "                \n",
    "                \n",
    "        import io\n",
    "        import re\n",
    "\n",
    "        # чистим текст\n",
    "        regex = re.compile(\"[^абвгдеёжзийклмнопрстуфхцчшщъыьэюя]\")\n",
    "        collector = []\n",
    "        poems_file = io.open(self.original_file, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "        if poems_file.mode == \"r\":\n",
    "            contents = poems_file.readlines()\n",
    "            for content in contents:\n",
    "                temp_collector = []\n",
    "                for word in content.split():\n",
    "                    new_word = regex.sub(\"\", word.lower())\n",
    "                    if new_word != \"\" and new_word != \" \":\n",
    "                        temp_collector.append(new_word)\n",
    "                collector.append(temp_collector)\n",
    "\n",
    "        self.new_file = self.original_file[:-4] + \"_clean.txt\"\n",
    "\n",
    "        with io.open(self.new_file, \"w\", encoding=\"utf-8\") as output:\n",
    "            temp_text = \" \"\n",
    "            for entry in collector:\n",
    "                new_poem = temp_text.join(entry)\n",
    "                output.write(new_poem + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde22a9",
   "metadata": {},
   "source": [
    "## Цепи\n",
    "\n",
    "метод \"Цепей Маркова\" является частично случайной и создание новых слова в последовательности зависит от того, какие слова использовались ранее\n",
    "\n",
    "- первые два слова стиха выбираются абсолютно случайно из всех возможных комбинаций слов\n",
    "- создаёт список всех комбинаций в которых использовались именно 2 предыдущих слова и оценивает какое слово может идти за ними\n",
    "- процесс повторяется, только теперь алгоритм оценивает все комбинации слов\n",
    "- файл MarkovChains.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c24df54",
   "metadata": {},
   "source": [
    "## Проблемы:\n",
    "\n",
    "1. Поиск рифм.\n",
    "Рассматривались различные варианты кодирования рифм, но я остановилась на форматах aabb, где a - предложения, формирующие первую рифму, b - предложения, формирующую вторую рифму и т.д. и свободный порядок, но гласное - наличие схожих букв в конце каких-то предложений.\n",
    "\n",
    "2. Технические шоколадки.\n",
    "    - Касательно формата aabb -\n",
    "Был найден Гутенберг корпус с поэзией, были разбиты предложения по рифмам aabb, abab, abba. Но для их обработки и обучения модели использовался PyTorch и, следовательно, CUDA. CUDA - это боль. Очень долго выдавало ошибку отсутсвия поддержкиб потом, на этапе тренировки модели, выпала фатальная ошибка - несовместимость с моей видеокартой. Так, эта неплохая идея с рифмами не успела реализоваться.\n",
    "\n",
    "    - Попытки обработки при помощи T5 -\n",
    "Опять же, ограничение технических возможностей.\n",
    "\n",
    "## Наработки (изначальный план):\n",
    "\n",
    "- Рифма\n",
    "Изначально была идея брать на вход текст (слово), используя сайт, определить ударение, например груша->грУша, затем при помощи RusVectores|Сайта синонимов искать близкие по смыслу слова и отбирать среди них слова с окончанием уша/юша.\n",
    "Определение рифмы в моем случае будет таким: созвучие заударной части слов и частичное совпадение самой ударной гласной. Т.е. кр[Ышка] - м[Ышка], зап[Ястье] - нен[Астье], [Ёлка] - мет[Ёлка]. \n",
    "А так же в строке должно быть одинаковое количество гласных. Были бы добавлены исключения типа \"нА реку\", \"пО лесу\", т.е. одно ударение на слово и предлог (т.к. одно фонетическое слово)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b37f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "груша\n",
      "грУша\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "from urllib import parse\n",
    "import requests\n",
    "import re\n",
    "\n",
    "x = input()\n",
    "URL = \"http://где-ударение.рф/в-слове-{0}\"\n",
    "\n",
    "\n",
    "def find_accent(x):\n",
    "    word = x.lower().strip()\n",
    "    _url = URL.format(word)\n",
    "    req = requests.get(_url)\n",
    "    page = bsoup(req.text, \"html.parser\")\n",
    "    if req.status_code == 200:\n",
    "        acc_rule = page.findChild(attrs={\"class\": \"rule\"})\n",
    "        result = re.search(r'[а-я]*<b>[А-ЯЁ]<\\/b>[а-я]*', str(acc_rule))\n",
    "        word_w_acc = result.group(0).replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n",
    "        return word_w_acc\n",
    "    return \"Nope\"\n",
    "\n",
    "word = x\n",
    "if x == x.lower():\n",
    "    word = find_accent(x)\n",
    "    print(word)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7931f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество слогов: 2\n"
     ]
    }
   ],
   "source": [
    "v = [\"а\", \"о\", \"э\", \"и\", \"ы\", \"у\", \"ю\", \"я\", \"е\", \"ё\"]\n",
    "v_count = 0\n",
    "for letter in word:\n",
    "    if letter.lower() in list(v):\n",
    "        v_count += 1\n",
    "print(\"количество слогов:\",v_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f175a3",
   "metadata": {},
   "source": [
    "- Генерация предложений\n",
    "Была идея создать несколько паттернов, описывающих структуру предложения, где V - verb, N - noun, PP - preposition, Conj - conjunctive, типа: \n",
    "    - N V PP N N\n",
    "    - Conj N V N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83168f7c",
   "metadata": {},
   "source": [
    "Мама купила на рынке ложку, а папа разбил плошку\n",
    "\n",
    "Для этого были использованы модули gensim и mystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d910b9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'растение, имя, фрукт, лампочка, вантуз, аграфена, азароль, дюшес, амбрет, дуля, агриппина, глива, бера, плюкалка, розоцветные, розоцветный, русле, бергамот, бере, бессемянка, дюшеска, малгоржатка, берре, виргулез, грушина, дулина, тыркалка, грушица, грушка, деканка, тонковетка, крассана, кюре, дерево, лимонка, клаппа, пневмогруша, семечковый'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#с сайта\n",
    "\n",
    "\n",
    "def get_words(page):\n",
    "    list_of_syn = []\n",
    "    element = page.findChild(attrs={\"class\": \"synonyms-table\"})\n",
    "    result = re.findall(r'[а-я]*<\\/a>', str(element))\n",
    "    for w in result:\n",
    "        w = w.replace(\"</a>\", \"\")\n",
    "        list_of_syn.append(w)\n",
    "    return list_of_syn\n",
    "\n",
    "def get_synonyms(word):\n",
    "    URL = \"https://synonyms.su/\"\n",
    "    word = word.strip().lower()\n",
    "    _url = URL + word[0].upper() + \"/\" + word\n",
    "    req = requests.get(_url)\n",
    "    page = bsoup(req.text, \"html.parser\")\n",
    "    words = \", \".join(get_words(page))\n",
    "    return words\n",
    "\n",
    "get_synonyms(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim & mystem\n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "import urllib.request\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "m = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
    "\n",
    "if m.endswith('.vec.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.KeyedVectors.load(m)\n",
    "    \n",
    "words = ['яблоко_S']\n",
    "\n",
    "for word in words:\n",
    "    # есть ли слово в модели? \n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(model[word][:10])\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print('Увы, слова \"%s\" нет в модели!' % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d93b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "\"\"\"\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием Mystem.\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tag_mystem(\n",
    "    text=\"Мама родная!\", mapping=None, postags=True\n",
    "):\n",
    " \n",
    "\n",
    "    processed = m.analyze(text)\n",
    "    tagged = []\n",
    "    for w in processed:\n",
    "        try:\n",
    "            lemma = w[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = w[\"analysis\"][0][\"gr\"].split(\",\")[0]\n",
    "            pos = pos.split(\"=\")[0].strip()\n",
    "            tagged.append(lemma.lower() + \"_\" + pos)\n",
    "        except KeyError:\n",
    "            continue  #пропускаю знаки препинания\n",
    "    if not postags:\n",
    "        tagged = [t.split(\"_\")[0] for t in tagged]\n",
    "    return tagged\n",
    "\n",
    "print(\"in process\")\n",
    "m = Mystem()\n",
    "\n",
    "print(\"input\")\n",
    "res = input()\n",
    "output = tag_mystem(text=res)\n",
    "word = \" \".join(output)\n",
    "if word in model:\n",
    "    for i in model.most_similar(positive=[word]):\n",
    "        print(i[0], i[1])\n",
    "    print('\\n')\n",
    "else:\n",
    "        # Увы!\n",
    "    print('Увы, слова \"%s\" нет в модели!' % word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e24b5",
   "metadata": {},
   "source": [
    "Еще был найдет модуль генерации текста с начала слова и на основе цепей Маркова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MarkovTextGenerator.markov_text_generator import MarkovTextGenerator\n",
    "\n",
    "\n",
    "\n",
    "my_generator = MarkovTextGenerator(2, None)\n",
    "my_generator.update(\"Есенин Сергей. Стихотворения. Поэмы - royallib.ru.txt\")\n",
    "my_generator.create_dump()\n",
    "\n",
    "print(my_generator.start_generation(\"Мы\"))#слово - то, с чего должно начинаться"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93cff7",
   "metadata": {},
   "source": [
    "Лучшее (и самое жизненное...), что он выдал:\n",
    "\n",
    "Мы не можем, что с вами случилось? Хочется вырвать окно и убежать в луг."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fb204",
   "metadata": {},
   "source": [
    "### Почему не продожила наработки?\n",
    "\n",
    "Идея создания паттернов показалась странной, тем более в процессе вылезли проблемы с форматами генсима и майстема - они разные. Позже я нашла код для их объединения. Слова-синонимы с сайта выглядят сомнительно, а как осуществить поиск по выдаче генсима определенной части речи для заполнения паттернов я не поняла.\n",
    "\n",
    "По итогу, если мне дадут возможность, я бы доработала эту версию, ну а пока есть только [aRahna.pythonanywhere.com](aRahna.pythonanywhere.com )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa13a8",
   "metadata": {},
   "source": [
    "## Спасибо за внимание!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
